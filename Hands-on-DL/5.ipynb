{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c81d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5da12",
   "metadata": {},
   "source": [
    "# 5.1 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19c40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb8aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0442,  0.1234, -0.3915,  0.0074, -0.1018, -0.0636,  0.1504,  0.0128,\n",
       "         -0.3646, -0.1830],\n",
       "        [ 0.0714,  0.2431, -0.2985,  0.0140, -0.1208, -0.0217, -0.0212,  0.0935,\n",
       "         -0.2427, -0.1419]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037fef8",
   "metadata": {},
   "source": [
    "#### 控制流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bfe2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3365, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "    \n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f3d17",
   "metadata": {},
   "source": [
    "#### 嵌套"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522de584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0533, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eafc10e",
   "metadata": {},
   "source": [
    "# 5.2 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8a16b",
   "metadata": {},
   "source": [
    "- 访问参数，用于调试、诊断和可视化；\n",
    "- 参数初始化；\n",
    "- 在不同模型组件间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ef2ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0072],\n",
       "        [ 0.0766]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf534b",
   "metadata": {},
   "source": [
    "## 5.2.1. 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "068586c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1599, -0.3518,  0.2931,  0.2840, -0.3372, -0.1144, -0.0933,  0.0117]])), ('bias', tensor([0.0932]))])\n",
      "<class 'torch.Tensor'> <class 'torch.nn.parameter.Parameter'>\n",
      "tensor([True]) tensor([0.0932])\n",
      "Parameter containing:\n",
      "tensor([0.0932], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())\n",
    "print(type(net[2].bias.data), type(net[2].bias))\n",
    "print(net[2].state_dict()['bias'] == net[2].bias.data, net[2].bias.data)\n",
    "print(net[2].bias)\n",
    "print(net[2].weight.grad == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dd27eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', torch.Size([8, 4])), ('bias', torch.Size([8]))]\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "net.state_dict()['2.bias'] == net.state_dict()['2.bias'].data == net[2].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cee68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3891,  0.0506,  0.2601, -0.4070,  0.4210, -0.0776, -0.4618, -0.3798])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "print(rgnet)\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97271a",
   "metadata": {},
   "source": [
    "## 5.2.2. 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c57ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0257, -0.0126, -0.0033,  0.0173],\n",
       "         [-0.0065,  0.0118, -0.0001, -0.0121],\n",
       "         [-0.0046, -0.0063, -0.0139, -0.0028],\n",
       "         [-0.0062,  0.0110,  0.0066,  0.0149],\n",
       "         [ 0.0096,  0.0086,  0.0038,  0.0007],\n",
       "         [ 0.0125,  0.0004,  0.0136, -0.0108],\n",
       "         [ 0.0019, -0.0095, -0.0109, -0.0123],\n",
       "         [ 0.0046, -0.0133, -0.0047, -0.0120]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data, net[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f2e0aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data, net[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c9e6456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5876, -0.0147, -0.3643,  0.2435])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a75518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.2204, -8.2161, -8.3685,  8.4781],\n",
       "        [ 6.6144,  0.0000,  0.0000, -6.1535]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aebd79ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  7.7839,  7.6315, 24.4781])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422e0ab",
   "metadata": {},
   "source": [
    "## 5.2.3. 参数绑定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47f5e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c686960",
   "metadata": {},
   "source": [
    "## 5.2.4. 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40d4f0",
   "metadata": {},
   "source": [
    "- 我们有几种方法可以访问、初始化和绑定模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77b94d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0375324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1398,  0.4255, -0.3753, -0.4328],\n",
      "        [-0.1223,  0.2621, -0.1239, -0.3444],\n",
      "        [ 0.1512, -0.2846, -0.3581,  0.3461],\n",
      "        [ 0.1768, -0.3991,  0.2120,  0.1912],\n",
      "        [ 0.1712, -0.1043, -0.4592, -0.3034],\n",
      "        [ 0.4076,  0.2136,  0.3750, -0.0874],\n",
      "        [ 0.1358,  0.3188, -0.2352,  0.4906],\n",
      "        [ 0.4113, -0.1312, -0.1755, -0.2966]]) \n",
      " tensor([-0.1001,  0.3579,  0.3047, -0.4429,  0.3814,  0.4992, -0.1454, -0.4104])\n",
      "tensor([[ 0.1398,  0.4255, -0.3753, -0.4328],\n",
      "        [-0.1223,  0.2621, -0.1239, -0.3444],\n",
      "        [ 0.1512, -0.2846, -0.3581,  0.3461],\n",
      "        [ 0.1768, -0.3991,  0.2120,  0.1912],\n",
      "        [ 0.1712, -0.1043, -0.4592, -0.3034],\n",
      "        [ 0.4076,  0.2136,  0.3750, -0.0874],\n",
      "        [ 0.1358,  0.3188, -0.2352,  0.4906],\n",
      "        [ 0.4113, -0.1312, -0.1755, -0.2966]]) \n",
      " tensor([-0.1001,  0.3579,  0.3047, -0.4429,  0.3814,  0.4992, -0.1454, -0.4104])\n",
      "weight Parameter containing:\n",
      "tensor([[ 0.1398,  0.4255, -0.3753, -0.4328],\n",
      "        [-0.1223,  0.2621, -0.1239, -0.3444],\n",
      "        [ 0.1512, -0.2846, -0.3581,  0.3461],\n",
      "        [ 0.1768, -0.3991,  0.2120,  0.1912],\n",
      "        [ 0.1712, -0.1043, -0.4592, -0.3034],\n",
      "        [ 0.4076,  0.2136,  0.3750, -0.0874],\n",
      "        [ 0.1358,  0.3188, -0.2352,  0.4906],\n",
      "        [ 0.4113, -0.1312, -0.1755, -0.2966]], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([-0.1001,  0.3579,  0.3047, -0.4429,  0.3814,  0.4992, -0.1454, -0.4104],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 访问\n",
    "print(net[0].weight.data, '\\n', net[0].bias.data)\n",
    "\n",
    "print(net[0].state_dict()['weight'], '\\n', net[0].state_dict()['bias'])\n",
    "\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d01b9360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "net[0].weight.data[:] = 1\n",
    "print(net[0].weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_42)\n",
    "print(net[0].weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722520c",
   "metadata": {},
   "source": [
    "# 5.4 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903a3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6221,  0.4359,  0.3277,  ..., -0.7431,  1.1807,  2.2402],\n",
      "        [ 0.4653, -0.0696, -1.4918,  ..., -0.7265, -0.2732, -1.7569],\n",
      "        [-0.8623, -0.3617, -2.4623,  ..., -0.1959, -0.3894,  0.5762],\n",
      "        ...,\n",
      "        [-0.1106,  1.7827,  0.8795,  ..., -2.0277,  1.6071,  0.0746],\n",
      "        [ 1.0450, -0.5560, -2.0071,  ..., -1.1902,  0.0043,  1.0860],\n",
      "        [-1.1276, -1.5355, -0.8339,  ...,  0.0780,  1.4438, -0.3149]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 28.6589,  -1.4791,  35.5858,   4.4783,  -0.7155, -34.6641,  -9.4101,\n",
       "          28.1011, -14.2645,  -7.7370],\n",
       "        [  9.4599,  -0.3365,   9.6549,  39.4441,  10.1339,  -8.9533,   1.5531,\n",
       "          -2.9039, -27.1742, -23.9092],\n",
       "        [  4.3896,  -0.9137,  34.6361, -53.7423,  26.1733,  19.3134, -31.6040,\n",
       "         -58.0925, -33.9551, -22.3526]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Mylinear(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input, output))\n",
    "        self.bias = nn.Parameter(torch.randn(output, ))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.weight) + self.bias\n",
    "\n",
    "X = torch.randn(3, 28*28)\n",
    "model = Mylinear(28*28, 10)\n",
    "print(model.weight.data)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d784ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.4149],\n",
       "        [-24.6522]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(Mylinear(64, 8), Mylinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a2fda",
   "metadata": {},
   "source": [
    "## 5.4.4. 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim, input_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        XXT = torch.mm(X, X.T)  # shape: (input_dim, input_dim)\n",
    "    \n",
    "        result = torch.sum(self.weight * XXT, dim=(1, 2))\n",
    "        # torch.einsum('ijk,jk->i', self.weight, XXT)\n",
    "        return result\n",
    "\n",
    "net = Down(20, 10)\n",
    "X = torch.randn(20, 20)\n",
    "print(net(X).shape)  # 输出: torch.Size([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a35457",
   "metadata": {},
   "source": [
    "# 5.5. 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3beee",
   "metadata": {},
   "source": [
    "## 5.5.2. 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b23c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "torch.save(net.state_dict(), 'mlp.params')\n",
    "\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "\n",
    "all(torch.equal(p1, p2) for p1, p2 in zip(net.parameters(), clone.parameters()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
